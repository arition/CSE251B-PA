\documentclass{article}

% if you need to pass options to natbib, use, e.g.: \PassOptionsToPackage{numbers, compress}{natbib} before loading
%     neurips_2020

% ready for submission \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the [preprint] option:
% \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.: \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[preprint]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfigure}

\title{Multi-layer Neural Networks on Fashion MNIST Dataset}

% The \author macro works with any number of authors. There are two commands used to separate the names and addresses of
% multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the lines. Using \AND forces a line break at
% that point. So, if LaTeX puts 3 of 4 authors names on the first line, and the last on the second line, try using \AND
% instead of \And before the third author name.

\author{%
  Fangzhou Ai\\
  Department of Electrical and Computer Engineering\\
  University of California, San Diego\\
  La Jolla, CA 92093 \\
  \texttt{faai@eng.ucsd.edu} \\
  % examples of more authors
   \And Yue Qiao \\
   Department of Electrical and Computer Engineering \\
   University of California, San Diego\\
   La Jolla, CA 92093 \\
   \texttt{yuq021@eng.ucsd.edu}\\
   \And Zunming Zhang \\
   Department of Electrical and Computer Engineering \\
   University of California, San Diego\\
   La Jolla, CA 92093 \\
   \texttt{zuz008@eng.ucsd.edu}\\
}

\begin{document}

\maketitle

\begin{abstract}
    This paper is a report for the programming assignment 2 for Lecture CSE 251B. In this report we build a simple
    neural network only relies on simple libraries like Numpy without any higher level machine-learning libraries
    invoked. This naive neural network can support multiple fully-connect layers and within each layer we can implement
    different activation functions, moreover, we also support regularization to prevent over-fitting from happening. We
    apply our model to the Fashion MNIST data-set and achieve expected precision. In our experiments, it shows that with
    0.001 L2 penalty and using ReLU as activation function, the network has best performance. 
\end{abstract}

\section{Gradient check}


\section{Implment the model}
We implemented our model on the Fashion MNIST dataset, we have 2 hidden layers here and each of them contains 50 nodes,
we apply the tanh activation function for those layers and softmax at the output layer, we spited the training set into
80/20 train/validation set, and did cross-validation making sure each part of data would be at least be the validation
set for 1 time, our batch size was 256, epochs limit was 300 here and the learning rate is 0.05, we also imposed a early
stop limit on our training, once the validation set error kept going up for more than 10 epochs, we stopped training
prcedure and recall the best parameters we saved before during the lowest validations set error, at the end of all
trainnigs, we achieved around 76.37\% accuracy. The loss and error curve are shown in \autoref{fig:loss_accuracy}.



\begin{figure}[ht]
    \centering
    \subfigure{
        \includegraphics[width=0.48\linewidth]{loss_c.png}
        \includegraphics[width=0.48\linewidth]{accu_c.png}
    }
    \caption{Traning and validation loss and accuracy for our model.}
    \label{fig:loss_accuracy}
\end{figure}

\section{Experiment with regularization}

We tried three different L2 penalty value: $0.01$, $0.001$, $0.0001$. After all, we find that $0.001$ is the best value
for our model. The loss and accuracy can be found in \autoref{fig:regularization}. The test set accuracy can be found in
\autoref{table:regularization}. We also find that with a smaller regularization value, more epoches are needed for
converge.

\begin{table}[ht]
    \caption{Test set accuracy for different regularization values}
    \label{table:regularization}
    \centering
    \begin{tabular}{@{}ll@{}}
        \toprule
        L2 penalty & Test set accuracy \\ \midrule
        0.01       & 0.7534            \\
        0.001      & \textbf{0.8560}   \\
        0.0001     & 0.8027            \\ \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[ht]
    \centering
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_d_0.01.png}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_d_0.001.png}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_d_0.0001.png}
    }
    \caption{Traning and validation loss and accuracy for different regularization values}
    \label{fig:regularization}
\end{figure}

\section{Experiment with activations}

We tried four different activation functions: sigmoid, tanh, ReLU and leakyReLU. Sigmoid is a nonlinear function that
helps the network archives nonlinearity. However, it tend to respond very less to changes of X and has a problem of
vanishing gradients. The derivatives of tanh are steeper than The derivatives of sigmoid, but it also has the vanishing
gradients problem. ReLU does not have vanishing gradient problem and is simplier to calculate. However, since the
gradient of ReLU for negative values are zero, some neurons will stop responding to the error. LeakyReLU tries to fix
the dead neurons problem, but since it introduces some kinds of linearity, it cannot be used on complex networks. In our
model, ReLU works best. LeakyReLU has similar performance compared to ReLU since our network is not complex enough. The
loss and accuracy can be found in \autoref{fig:activations}. The test set accuracy can be found in
\autoref{table:activations}.

\begin{table}[ht]
    \caption{Test set accuracy for different activations}
    \label{table:activations}
    \centering
    \begin{tabular}{@{}ll@{}}
        \toprule
        Activation & Test set accuracy \\ \midrule
        sigmoid    & 0.7884            \\
        tanh       & 0.8560            \\
        ReLU       & \textbf{0.8634}   \\
        LeakyReLU  & 0.8623            \\ \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[ht]
    \centering
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_e_sigmoid.png}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_e_tanh.png}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_e_ReLU.png}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_e_leakyReLU.png}
    }
    \caption{Traning and validation loss and accuracy for different activations}
    \label{fig:activations}
\end{figure}

\section{Experiment with Network Topology}
In this part, we used the best parameters from the previous experiment. To be specific, the parameters are listed below:
\begin{item}
	\itemize Learning rate = 0.01
	\itemize Amount of regularization \gamma = 0.001
	\itemize Activation function used: ReLU
\end{item}


\section{Individual contributions to the project}
\label{Sec:ICP}

\paragraph{Fangzhou Ai} Part (c)

\paragraph{Yue Qiao}

Experiment with Regularization and Experiment with Activations.

\paragraph{Zunming Zhang}
Implement the network, check the numerical approximation of weights and experiment with Network Topology

\end{document}