\documentclass{article}

% if you need to pass options to natbib, use, e.g.: \PassOptionsToPackage{numbers, compress}{natbib} before loading
%     neurips_2020

% ready for submission \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the [preprint] option:
% \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.: \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[preprint]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfigure}

\title{Multi-layer Neural Networks on Fashion MNIST Dataset}

% The \author macro works with any number of authors. There are two commands used to separate the names and addresses of
% multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the lines. Using \AND forces a line break at
% that point. So, if LaTeX puts 3 of 4 authors names on the first line, and the last on the second line, try using \AND
% instead of \And before the third author name.

\author{%
  Fangzhou Ai\\
  Department of Electrical and Computer Engineering\\
  University of California, San Diego\\
  La Jolla, CA 92093 \\
  \texttt{faai@eng.ucsd.edu} \\
  % examples of more authors
   \And Yue Qiao \\
   Department of Electrical and Computer Engineering \\
   University of California, San Diego\\
   La Jolla, CA 92093 \\
   \texttt{yuq021@eng.ucsd.edu}\\
   \And Zunming Zhang \\
   Department of Electrical and Computer Engineering \\
   University of California, San Diego\\
   La Jolla, CA 92093 \\
   \texttt{zuz008@eng.ucsd.edu}\\
}

\begin{document}

\maketitle

\begin{abstract}
    This paper is a report for the programming assignment 2 for Lecture CSE 251B. In this report we build a simple
    neural network only relies on simple libraries like Numpy without any higher level machine-learning libraries
    invoked. This naive neural network can support multiple fully-connect layers and within each layer we can implement
    different activation functions, moreover, we also support regularization to prevent over-fitting from happening. We
    apply our model to the Fashion MNIST data-set and achieve expected precision. In our experiments, it shows that with
    0.001 L2 penalty and using ReLU as activation function, the network has best performance. 
\end{abstract}

\section{Gradient check}
In this part, we randomly chose one output bias weight and one hidden bias weight from each hidden layer and checked the differences from what we obtained from the back-propagation and numerical approximation. For the weight, we randomly chose two input to hidden weight and two hidden to output weights and compared these from what we obtained from the back-propagation and numerical approximation. The results are shown in \autoref{table:gradient}.

\begin{table}[ht]
    \caption{Comparison of bias weight and weights from back-propagation and numerical approximation}
    \label{table:gradient}
    \centering
    \begin{tabular}{@{}ll@{}ll@{}}
        \toprule
                                                    & Back-propagation           & Numerical approximation \\ \midrule
        output bias weight                          & 0.2829988269               & 0.2829986440                 \\
        bias weight from first hidden layer         & 0.0947910547               & 0.0947903741                 \\
        bias weight from second hidden layer        & 0.1328027731               & 0.1327997287                 \\
        weight 1 from input to hidden layer         & -0.07680517785             & -0.0768048158                \\
        weight 2 from input to hidden layer         & 0.01531950744              & 0.0153201236                 \\
        weight 1 from hidden to hidden layer        & -0.13117408499             & -0.1311709672                \\
        weight 2 from hidden to hidden layer        & 0.00568704342              & 0.0056867596                 \\
        weight 1 from hidden to output layer        & 0.26639110994              & 0.2663909043                 \\
        weight 2 from hidden to output layer        & -0.09570147055             & -0.0957014082                \\ \bottomrule
    \end{tabular}
\end{table}

From \autoref{table:gradient}, we could see that the differences from the back-propagation and from numerical approximation indeed is within $O\left(\epsilon^2\right)$.

\section{Implment the model}
We implemented our model on the Fashion MNIST dataset, we have 2 hidden layers here and each of them contains 50 nodes,
we apply the tanh activation function for those layers and softmax at the output layer, we spited the training set into
80/20 train/validation set, and did cross-validation making sure each part of data would be at least be the validation
set for 1 time, our batch size was 256, epochs limit was 300 here and the learning rate is 0.05, we also imposed a early
stop limit on our training, once the validation set error kept going up for more than 10 epochs, we stopped training
prcedure and recall the best parameters we saved before during the lowest validations set error, at the end of all
trainnigs, we achieved around 76.37\% accuracy. The loss and error curve are shown in \autoref{fig:loss_accuracy}.



\begin{figure}[ht]
    \centering
    \subfigure{
        \includegraphics[width=0.48\linewidth]{loss_c.png}
        \includegraphics[width=0.48\linewidth]{accu_c.png}
    }
    \caption{Traning and validation loss and accuracy for our model.}
    \label{fig:loss_accuracy}
\end{figure}

\section{Experiment with regularization}

We tried three different L2 penalty value: $0.01$, $0.001$, $0.0001$. After all, we find that $0.001$ is the best value
for our model. The loss and accuracy can be found in \autoref{fig:regularization}. The test set accuracy can be found in
\autoref{table:regularization}. We also find that with a smaller regularization value, more epoches are needed for
converge.

\begin{table}[ht]
    \caption{Test set accuracy for different regularization values}
    \label{table:regularization}
    \centering
    \begin{tabular}{@{}ll@{}}
        \toprule
        L2 penalty & Test set accuracy \\ \midrule
        0.01       & 0.7534            \\
        0.001      & \textbf{0.8560}   \\
        0.0001     & 0.8027            \\ \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[ht]
    \centering
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_d_0.01.png}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_d_0.001.png}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_d_0.0001.png}
    }
    \caption{Traning and validation loss and accuracy for different regularization values}
    \label{fig:regularization}
\end{figure}

\section{Experiment with activations}

We tried four different activation functions: sigmoid, tanh, ReLU and leakyReLU. Sigmoid is a nonlinear function that
helps the network archives nonlinearity. However, it tend to respond very less to changes of X and has a problem of
vanishing gradients. The derivatives of tanh are steeper than The derivatives of sigmoid, but it also has the vanishing
gradients problem. ReLU does not have vanishing gradient problem and is simplier to calculate. However, since the
gradient of ReLU for negative values are zero, some neurons will stop responding to the error. LeakyReLU tries to fix
the dead neurons problem, but since it introduces some kinds of linearity, it cannot be used on complex networks. In our
model, ReLU works best. LeakyReLU has similar performance compared to ReLU since our network is not complex enough. The
loss and accuracy can be found in \autoref{fig:activations}. The test set accuracy can be found in
\autoref{table:activations}.

\begin{table}[ht]
    \caption{Test set accuracy for different activations}
    \label{table:activations}
    \centering
    \begin{tabular}{@{}ll@{}}
        \toprule
        Activation & Test set accuracy \\ \midrule
        sigmoid    & 0.7884            \\
        tanh       & 0.8560            \\
        ReLU       & \textbf{0.8634}   \\
        LeakyReLU  & 0.8623            \\ \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[ht]
    \centering
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_e_sigmoid.png}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_e_tanh.png}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_e_ReLU.png}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{part_e_leakyReLU.png}
    }
    \caption{Training and validation loss and accuracy for different activation functions}
    \label{fig:activations}
\end{figure}

\section{Experiment with Network Topology}
In this part, we used the best parameters from the previous experiment. To be specific, the parameters that used in this part are listed below:
\begin{itemize}
	\item Learning rate = 0.01
	\item Amount of regularization $\gamma$ = 0.001
	\item Activation function used: ReLU
\end{itemize}
\subsection{Experiment with different hidden unit number in each layer}
First, tried to vary the number of the hidden units in each layer. We tried the combination of 50 X 100, 100 X 50, 25 X 50, and 50 X 25. We tried to double/halve hidden units in one of the hidden layers. And the test set accuracy could be found in \autoref{table:unitnumber}. Basically, increasing the hidden unit number typically means that increase the network complexity and suppose to get a better performance. However, from \autoref{table:unitnumber}, we could see that the performances remain almost the same when we are trying to increase the hidden unit number of one of the hidden layers. Decreasing the hidden unit number would normally decrease the complexity of the network. We could see that the test set accuracy decrease by around $1\%-2\%$ when we reduced the network complexity. 
\begin{table}[ht]
    \caption{Test set accuracy for different number of hidden units in each layer}
    \label{table:unitnumber}
    \centering
    \begin{tabular}{@{}ll@{}}
        \toprule
        Hidden unit number in each layer & Test set accuracy \\ \midrule
        50 $X$ 50        & 0.8634             \\
        50 $X$ 100       & 0.8622             \\
        100 $X$ 50       & 0.8629    \\
        25 $X$ 50        & 0.8484             \\
        50 $X$ 25        & 0.8584             \\ \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[ht]
    \centering
    \subfigure{
        \includegraphics[width=0.96\linewidth]{Part_f_01.JPG}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{Part_f_02.JPG}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{Part_f_03.JPG}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{Part_f_04.JPG}
    }
    \caption{Training and validation loss and accuracy for different hidden units}
    \label{fig:hiddenunit}
\end{figure}

\subsection{Experiment with different number of hidden layer}
In this section, we were exploring the performance changes of different hidden layer number while we kept the overall weights and biases number to be roughly the same. The total weights number for the two hidden layers structure is $784*50 + 50*50 + 50*10 = 42200$ and the bias number for the hidden layers is $50 + 50 = 100$.So, for the three hidden layer, we used [50, 30, 30] hidden unit in each layer and for the one hidden layer, we used 100 hidden unit and tried to compared the results of each network. The test set accuracy can be found in \autoref{table:layernumber}. As we could see in the \autoref{table:layernumber}, the test set accuracies are comparable for the three different structures.

\begin{table}[ht]
    \caption{Test set accuracy for different number of hidden layer}
    \label{table:layernumber}
    \centering
    \begin{tabular}{@{}ll@{}}
        \toprule
        Layer specs & Test set accuracy \\ \midrule
        50 $X$ 50        & 0.8634             \\
        50 $X$ 30 $X$ 30 & 0.8641             \\
        100              & 0.8552    \\ \bottomrule
    \end{tabular}
\end{table}
\begin{figure}[ht]
    \centering
    \subfigure{
        \includegraphics[width=0.96\linewidth]{Part_f_3.JPG}
    }
    \subfigure{
        \includegraphics[width=0.96\linewidth]{Part_f_1.JPG}
    }
    \caption{Training and validation loss and accuracy for different hidden layer}
    \label{fig:hiddenlayer}
\end{figure}

\section{Individual contributions to the project}
\label{Sec:ICP}

\paragraph{Fangzhou Ai} Part (c)

\paragraph{Yue Qiao}

Experiment with Regularization and Experiment with Activations.

\paragraph{Zunming Zhang}
Implement the network, check the numerical approximation of weights and experiment with Network Topology

\end{document}