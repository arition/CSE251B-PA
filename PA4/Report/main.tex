\documentclass{article}

% if you need to pass options to natbib, use, e.g.: \PassOptionsToPackage{numbers, compress}{natbib} before loading
%     neurips_2020

% ready for submission \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the [preprint] option:
% \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.: \usepackage[final]{neurips_2020}

\PassOptionsToPackage{numbers, compress}{natbib}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[preprint]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\title{Image Captioning using an LSTM network}

% The \author macro works with any number of authors. There are two commands used to separate the names and addresses of
% multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the lines. Using \AND forces a line break at
% that point. So, if LaTeX puts 3 of 4 authors names on the first line, and the last on the second line, try using \AND
% instead of \And before the third author name.

\author{%
  Fangzhou Ai\\
  Department of Electrical and Computer Engineering\\
  University of California, San Diego\\
  La Jolla, CA 92093 \\
  \texttt{faai@eng.ucsd.edu} \\
  % examples of more authors
   \And Yue Qiao \\
   Department of Electrical and Computer Engineering \\
   University of California, San Diego\\
   La Jolla, CA 92093 \\
   \texttt{yuq021@eng.ucsd.edu}\\
   \And Zunming Zhang \\
   Department of Electrical and Computer Engineering \\
   University of California, San Diego\\
   La Jolla, CA 92093 \\
   \texttt{zuz008@eng.ucsd.edu}\\
}

\begin{document}

\maketitle

\begin{abstract}
  Image captioning is an important problem in artificial intelligence, related to both computer vision and natural language processing. 
This work explores the power of Recurrent Neural Networks to deal with image captioning problem. 
We used a pre-trained convolutional network, ResNet50, as the encoder and an LSTM model as the decoder. 
We also tried to replaced the LSTM with Vinalla RNN and GRU and compared the performances of different models.
For evaluation, we tried to compare the test set Cross Entropy Loss and BLEU scores for different models.
For baseline model, we achieved a test loss of 1.966, BLEU1 socre of 56.48 and BLEU4 score of 7.75. 
We got a better result for GRU method and the comparison could be found in \autoref{table:CrossEntropyLoss} and \autoref{table:BLEUScores}. 
We also tried to fine tuned the baseline model and we successfully increased the BLEU1 to 65.02, BLEU4 to 16.83 and
decreased the loss to 1.517.
\end{abstract}

%--------------------------------------------

\section{Introduction}

Automatically generating a natural language description of an image, a problem known as image captioning, has recently
received a lot of attention in Computer Vision. The problem is interesting not only because it has important practical
applications, such as helping visually impaired people see, but also because it is regarded as a grand challenge for
image understanding which is a core problem in Computer Vision. Generating a meaningful natural language description of
an image requires a level of image understanding that goes well beyond image classification and object detection. The
problem is also interesting in that it connects Computer Vision with Natural Language Processing which are two major
fields in Artificial Intelligence.

In this assignment, we will explore the power of Recurrent Neural Networks to deal with data that has temporal
structure. Specifically, we will generate captions for images. In order to achieve this, we will need an encoder-decoder
architecture for this assignment. Simply, the encoder will take the image as input and encode it into a vector of
feature values. This will then be passed through a linear layer for providing the input to LSTM. It will be trained
to predict the next word at each step. we used a pre-trained convolutional network as the encoder and an LSTM model as
the decoder and tried to fine tune the encoder and train the decoder by backpropagating error into it. The error will
come from caption generation. The training uses images and several captions for each image generated by humans. We then
run the network in generative mode to generate captions on images it has never seen before. We also tried to replace the
LSTM in the encoder part with Vanilla RNN and GRU and compared the performances of them.


%--------------------------------------------

\section{Related Work}
Recently, several approaches have been proposed for image captioning. We can roughly classify those methods into three
categories. The first category is template based approaches that generate caption templates based on detecting objects
and discovering attributes within image. For example, the work \cite{li2011composing} was proposed to parse a whole
sentence into several phrases, and learn the relationships between phrases and objects within an image. In
\cite{kulkarni2013babytalk}, conditional random field (CRF) was used to correspond objects, attributes and prepositions
of image content and predict the best label. Other similar methods were presented in \cite{mitchell2012midge,
  kuznetsova2014treetalk, kuznetsova2012collective}. These methods are typically hard-designed and rely on fixed template,
which mostly lead to poor performance in generating variable-length sentences. The second category is retrieval based
approach, this sort of methods treat image captioning as retrieval task. By leveraging distance metric to retrieve
similar captioned images, then modify and combine retrieved captions to generate caption \cite{kuznetsova2014treetalk}.
But these approaches generally need additional procedures such as modification and generalization process to fit image
query.

Inspired by the success use of CNN \cite{krizhevsky2012imagenet, zeiler2014visualizing} and Recurrent Neural Network
\cite{bahdanau2014neural, mikolov2010recurrent, mikolov2011extensions}. The third category is emerged as neural network
based methods \cite{karpathy2014deep, karpathy2015deep, kiros2014unifying, vinyals2015show, xu2015show}. Our work also
belongs to this category. Among those work, Kiro et al.\cite{kiros2014multimodal} can be as pioneer work to use neural
network for image captioning with multimodal neural language model. In their follow up work \cite{kiros2014unifying},
Kiro et al. introduced an encoder-decoder pipeline where sentence was encoded by LSTM and decoded with structure-content
neural language model (SC-NLM). Socher et al.\cite{socher2014grounded} presented a DT-RNN (Dependency Tree-Recursive
Neural Network) to embed sentence into a vector space in order to retrieve images. Later on, Mao et
al.\cite{mao2014deep} proposed m-RNN which replaces feed-forward neural language model in \cite{kiros2014unifying}.
Similar architectures were introduced in NIC \cite{vinyals2015show} and LRCN \cite{donahue2015long}, both approaches use
LSTM to learn text context.

Our work is based on LSTM method to do the image captioning. In our approach, we used a pre-trained concolutional
network as the encoder and an LSTM model as the decoder to do the image captioning. We also tried to replaced LSTM
with the Vanilla RNN and GRU to compared the model performances.
%--------------------------------------------

\section{Methods}


\subsection{Dataset}
For this image captioning task, we used the dataset from the well-known Common Objects in Context\cite{lin2015microsoft}
(COCO) repssitory. COCO is a large-scale object detection, segmentation, and captioning dataset. In this report, we used
a subset (around 1/5) of the COCO 2015 Image Captioning Task. The training set contains around 82k images with roughly
410k captions while the test set has around 3k images with almost 15k captions. The original images in the dataset are
of different sizes and aspect ratios, which we are resizing to 256x256 before the training.


\subsection{Model}

\subsubsection{Baseline Model}
Our captioning system is implemented based on a Long Short-Time Memory (LSTM) network (baseline model). For the encoder
part, we use a forzen pretrained convolutional network, namely ResNet50, as the encoder. We removed the last layer of
the network and added a trainable linear layer that outputs a feaure vector with a fixed size for each image. This
feature becomes the initial hidden state and the cell state of LSTM network. For baseline model, we resized the
image to 256x256.

For LSTM decoder, we first initialize the hidden state and the cell state using the encoded image from CNN encoder.
Then, we send all captions, seperated by space, into the embedding layer. The embedding layer basicly converts the words
feature into one hot encoding format and reduces the feature dimensions so that the features fit LSTM network's
input size. Later, we feed the embedded features into LSTM cells. Instead of using the output from the last LSTM
cell, we use a techique called teacher forcing, which feed the network with the ground truth last word regardless of
what the network outputs last time. The outputs of LSTM cell will feed to a linear layer to scale up the dimensions.
In the end, the scaled output features are feed into the softmax layer.

\subsubsection{Vanilla RNN and GRU}
For the model comparison, we also tried Vanilla RNN and GRU\cite{cho2014learning}. GRU is a varient of recurrent neural
networks. GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it
lacks an output gate.\cite{enwiki:997015931} For these two models, we simply replaced LSTM module in the encoder
with either a Vanilla or a GRU, while the others remained to be same. Then, trained and compared the performance of
these three different models.



%--------------------------------------------

\section{Results}

For best fine-tuning parameter, we use hidden size as 1200 and embedding size as 2000.
\subsection{Learning Curve}

The learning curve for LSTM, Vanilla RNN and GRU could be found in \autoref{fig:loss_train_val_model}.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{LSTM_Loss.png}
    \caption{LSTM Loss}
    \label{fig:loss_LSTM}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{VanillaRNN_Loss.png}
    \caption{Vanilla RNN Loss}
    \label{fig:loss_RNN}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{GRU_Loss.png}
    \caption{GRU Loss}
    \label{fig:loss_GRU}
  \end{subfigure}
  \caption{Training and Validation loss for LSTM, Vanilla RNN and GRU}
  \label{fig:loss_train_val_model}
\end{figure}



\subsection{Cross Entropy Loss}
The test set Cross Entropy Loss of different models could be found in \autoref{table:CrossEntropyLoss}

\begin{table}[h]
  \caption{Cross Entropy Loss for different models}
  \label{table:CrossEntropyLoss}
  \centering
  \begin{tabular}{@{}ll@{}}
    \toprule
    Model             & Testset Cross Entropy Loss \\
    \midrule
    Baseline          & 1.966                      \\
    Vanilla RNN       & 1.819                      \\
    GRU               & 1.586                      \\
    Fine tuning model & 1.517                      \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{BLEU Score}
For this report, we set the weight for BLEU1 to be [1, 0, 0, 0] and BLEU4 to be [0.25, 0.25, 0.25, 0.25].
The BLEU scores for different models could be found in \autoref{table:BLEUScores}

\begin{table}[h]
  \caption{BLEU scores for different models}
  \label{table:BLEUScores}
  \centering
  \begin{tabular}{@{}lll@{}}
    \toprule
    Model              & BLEU1 & BLEU4 \\
    \midrule
    Baseline           & 56.48 & 7.75  \\
    Vanilla RNN        & 54.59 & 8.93  \\
    GRU                & 65.16 & 17.17 \\
    Finee tuning model & 65.02 & 16.83 \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Images Visualization of the Best Performance Model}
The good predictions are shwon in \autoref{fig:good_pred} and bad predictions are shown in \autoref{fig:bad_pred}.
\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{g1.jpg}
	\caption{Predicted:a bathroom with a toilet, sink and shower. True:A bathroom that has a tub, a toilet, a sink, and a mirror.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{g2.jpg}
	\caption{Predicted:a woman sitting under an umbrella. True:A girl sits in a chair on a porch in summer.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{g3.jpg}
	\caption{Preditced:a herd of sheep walking down a dirt road. True:A large herd of sheep crossing the road in front of a van.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{g4.jpg}
	\caption{Predicted:a living room filled with furniture and a flat screen tv. True:A living area with a group of sofas, coffee table and television.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{g5.jpg}
	\caption{Predicted:a little girl sitting at a table with a plate of food. True:That piece of cake will soon be eaten by the little girl.}
	\label{fig:loss_GRU}
\end{subfigure}
	\caption{Good predictions.}
	\label{fig:good_pred}
\end{figure}

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{b1.jpg}
		\caption{Predicted:a train on a train track near a building. True: A commuter train pulling out of a suburban station.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{b2.jpg}
		\caption{Preditced:A man standing in front of a refrigerator. True:a half bathroom door showing the bottom half of someones legs}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{b3.jpg}
		\caption{Predicted: person riding a horse on a beach. True:a man with a hat and a back pack riding on a horse.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{b4.jpg}
		\caption{Predicted:a parking meter on the side of the road. True:Two parking meters are on top of a pole.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{b5.jpg}
		\caption{Predicted:a man is holding a cell phone in his hand. True:a young man in a grey shirt is going to cut his hair.}
		\label{fig:loss_GRU}
	\end{subfigure}
	\caption{Bad predictions.}
	\label{fig:bad_pred}
\end{figure}
%--------------------------------------------


\section{Discussion}

\subsection{Baseline with different Temperatures}

We try to use a stochastic approach and generate the output words using the distribution from softmax layer. The
temperature controls what the distribution looks like. When the temperature approaches $0$, the distribution is nearly
determinstic. When the temperature approaches $\infty$, the distribution is completely uniform. The BLEU scores is
listed in \autoref{table:StochasticBLEUScores}.

\begin{table}[h]
  \caption{BLEU scores for baseline model with different temperature}
  \label{table:StochasticBLEUScores}
  \centering
  \begin{tabular}{@{}lll@{}}
    \toprule
    Temperature & BLEU1 & BLEU4 \\
    \midrule
    0.01        & 0.117 & 0.022 \\
    0.1         & 0.107 & 0.020 \\
    0.2         & 0.099 & 0.018 \\
    0.7         & 0.106 & 0.020 \\
    1           & 0.118 & 0.022 \\
    1.5         & 0.142 & 0.026 \\
    2           & 0.152 & 0.028 \\
    5           & 0.169 & 0.032 \\
    \bottomrule
  \end{tabular}
\end{table}

From the table we can see that a higher temperature produces higher BLEU score. All of the score is lower than the
determinstic approach. We suppose that the baseline model is too small to support the stochastic approach to produce
good sentences.

\subsection{Baseline LSTM vs Vanilla RNN vs GRU}

\subsubsection{Learning Curve}
From the \autoref{fig:loss_train_val_model}, we can see that the Vanilla RNN takes 110 epochs to converge, LSTM takes
about 80 epochs to converge, and GRU also takes about 80 epochs to converge. We believe that since RNN does not contains
any kinds of forgot gate, the model suffers from vanishing gradients problem and the training is slower.

\subsubsection{Cross Entropy Loss}
Compare with the loss value in \autoref{table:CrossEntropyLoss}, we can see that GRU has a lower loss. It proves that
GRU works better on networks with small feature sizes.

\subsubsection{BLEU Score}
Compare with the BLEU score value in \autoref{table:BLEUScores}, we can see that the Vanilla RNN is worse than LSTM and
GRU on BLEU-1 score. GRU has the highest score on both BLEU-1 and BLEU-4. It again shows that GRU works better on
networks with small feature sizes.

\subsection{Baseline model vs fine tuning model}
For fine tuning part, we use grid search method to find the best model, all tasks are submitted to GPU cluster as batch jobs. The result shows that increasing the embedding size is mode effective than increasing the hidden size, which is reasonable since the length of the vector for each word is 60,000 in this dataset, the original 300 embedding size is apparently too small to fit large vocabulary. In our experiment we found that when hidden size is 1200 and embedding size is 2000, we achieve the best BLEU score.
\subsubsection{Learning Curve}
The learning curve is shown in \autoref{fig:loss_FT}.
\begin{figure}
	\centering
	\includegraphics[width=8cm]{Fine_tuning_loss.png}
	\caption{Fine tuning model loss.}
	\label{fig:loss_FT}
\end{figure}

\subsubsection{Cross Entropy Loss}
The loss for each model with different parameters are shown in \autoref{table:FT_val_loss}. All the results are collected after 100 epochs training. The validation loss shows that larger embedding size is better than larger hidden size.
\begin{table}[h]
	\caption{Validation loss of different models.}
	\label{table:FT_val_loss}
	\centering
	\begin{tabular}{@{}lllllll@{}}
		\toprule
		Hidden/embedding size & 1000 & 1200 & 1400 & 1600 & 1800 & 2000 \\
		\midrule
		1000       & 1.681337369	& 1.691209305 &	1.580959492 & 1.551394041 & 1.583891191 & 1.560777765\\
		1200        &1.816109741 & 1.618265422	&1.57035513	&1.536661038	&1.524779346	&1.522998657 \\
		1400       &1.662981744	&1.641106087	&1.616746906	&1.559747254	&1.637271137	&1.543163765 \\
		1600         &1.75068753&	1.587794039&	1.563622826&	1.529088398& 1.550765577&	1.519091342\\
		1800           &1.646753683	&1.591480875&	1.586649266&	1.591782318&	1.537952337&	1.521097336\\
		2000         &1.639377112	&1.586326071	&1.671221515&	1.595450379	&1.560300544	&1.517007219\\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection{BLEU Score}
The BLEU1 scores are shown in \autoref{table:BLEU1_FT} and BLEU4 scores are shown in \autoref{table:BLEU4_FT}. We can also see that larger embedding size is better than larger hidden size.
\begin{table}[h]

	\caption{BLEU1 score of different models.}
	\label{table:BLEU1_FT}
	\centering
	\begin{tabular}{@{}lllllll@{}}
		\toprule
		Hidden/embedding size & 1000 & 1200 & 1400 & 1600 & 1800 & 2000 \\
		\midrule
		1000       & 59.41769621&	59.62020083&	63.35089825&	63.32872151&	63.73170132&	64.43839568\\
		1200        &56.05131035	&62.30367823&	62.35710535&	63.26050116&	64.60560355&	65.01930785 \\
		1400       &60.39761923	&60.39945857&	60.63426329&	62.73186609&	61.13974605&	64.93391073\\
		1600         &57.34373218	&61.60656617&	62.77856346&	62.47185297&	64.2897595&	63.74235525\\
		1800           &58.97915755	&61.51667164	&62.34537701	&60.52766041	&63.5199832&	63.17110813\\
		2000         &59.78645267&	60.58827569&	58.05156176&	61.15798722	&62.67753716&	63.65793274\\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[h]
	
	\caption{BLEU4 score of different models.}
	\label{table:BLEU4_FT}
	\centering
	\begin{tabular}{@{}lllllll@{}}
		\toprule
		Hidden/embedding size & 1000 & 1200 & 1400 & 1600 & 1800 & 2000 \\
		\midrule
		1000       & 11.84089542	&12.40819432	&15.3686782	&15.82054416&	15.70298138&	16.54373736\\
		1200        &8.772820601	&14.55022853	&15.01156252	&15.60198284	&16.4523467	&16.83235187\\
		1400       &12.8796874	&12.91051223	&13.49639036	&15.16698644	&13.21635365	&16.18174622\\
		1600         &10.58407818&	14.31981595&	14.94549483&	15.37257563&	16.52898121&	15.7704121\\
		1800         &12.36232193	&14.22257743&	14.84009182&	13.30323795&	15.66635975&	15.41644636\\
		2000         &12.19404039&	13.25380803&	11.32768971&	13.89880156&	15.30576858&	15.89286523\\
		\bottomrule
	\end{tabular}
\end{table}





\subsubsection{Fine tuning for Vanilla RNN and GRU}
Though this is not required, we still tuned the vanilla RNN and GRU model to check the differences between these tow models and LSTM. During these extra fine tuning process, we also observed that the embedding size has a more important effect on validation loss and BLEU score than hidden size, the difference is RNN and GRU model seems to be saturated when embedding size is around 1200 ~ 1400, namely after that increasing embedding size cannot bring us more improvement, while for LSTM, the saturation threshold is approaching 2000. 






%--------------------------------------------

\section{Individual contributions to the project}

\paragraph{Fangzhou Ai}

Fine tuning.
\paragraph{Yue Qiao}

Baseline LSTM Model.

\paragraph{Zunming Zhang}
Vanilla and GRU model.

\bibliographystyle{plainnat}
\bibliography{bibfile}

\end{document}