\documentclass{article}

% if you need to pass options to natbib, use, e.g.: \PassOptionsToPackage{numbers, compress}{natbib} before loading
%     neurips_2020

% ready for submission \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the [preprint] option:
% \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.: \usepackage[final]{neurips_2020}

\PassOptionsToPackage{numbers, compress}{natbib}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[preprint]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\title{Image Captioning using an LSTM network}

% The \author macro works with any number of authors. There are two commands used to separate the names and addresses of
% multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the lines. Using \AND forces a line break at
% that point. So, if LaTeX puts 3 of 4 authors names on the first line, and the last on the second line, try using \AND
% instead of \And before the third author name.

\author{%
  Fangzhou Ai\\
  Department of Electrical and Computer Engineering\\
  University of California, San Diego\\
  La Jolla, CA 92093 \\
  \texttt{faai@eng.ucsd.edu} \\
  % examples of more authors
   \And Yue Qiao \\
   Department of Electrical and Computer Engineering \\
   University of California, San Diego\\
   La Jolla, CA 92093 \\
   \texttt{yuq021@eng.ucsd.edu}\\
   \And Zunming Zhang \\
   Department of Electrical and Computer Engineering \\
   University of California, San Diego\\
   La Jolla, CA 92093 \\
   \texttt{zuz008@eng.ucsd.edu}\\
}

\begin{document}

\maketitle

\begin{abstract}
  In this report, we ...
\end{abstract}

%--------------------------------------------

\section{Introduction}

Automatically generating a natural language description of an image, a problem known as image captioning, has recently
received a lot of attention in Computer Vision. The problem is interesting not only because it has important practical
applications, such as helping visually impaired people see, but also because it is regarded as a grand challenge for
image understanding which is a core problem in Computer Vision. Generating a meaningful natural language description of
an image requires a level of image understanding that goes well beyond image classification and object detection. The
problem is also interesting in that it connects Computer Vision with Natural Language Processing which are two major
fields in Artificial Intelligence.

In this assignment, we will explore the power of Recurrent Neural Networks to deal with data that has temporal
structure. Specifically, we will generate captions for images. In order to achieve this, we will need an encoderdecoder
architecture for this assignment. Simply put, the encoder will take the image as input and encode it into a vector of
feature values. This will then be passed through a linear layer for providing the input to the LSTM. It will be trained
to predict the next word at each step. we used a pre-trained convolutional network as the encoder and an LSTM model as
the decoder and tried to fine tune the encoder and train the decoder by backpropagating error into it. The error will
come from caption generation. The training uses images and several captions for each image generated by humans. We then
run the network in generative mode to generate captions on images it has never seen before. We also tried to replace the
LSTM in the encoder part with Vanilla RNN and GRU and compared the performances of them.


%--------------------------------------------

\section{Related Work}
Recently, several approaches have been proposed for image captioning. We can roughly classify those methods into three
categories. The first category is template based approaches that generate caption templates based on detecting objects
and discovering attributes within image. For example, the work \cite{li2011composing} was proposed to parse a whole
sentence into several phrases, and learn the relationships between phrases and objects within an image. In
\cite{kulkarni2013babytalk}, conditional random field (CRF) was used to correspond objects, attributes and prepositions
of image content and predict the best label. Other similar methods were presented in \cite{mitchell2012midge,
  kuznetsova2014treetalk, kuznetsova2012collective}. These methods are typically hard-designed and rely on fixed template,
which mostly lead to poor performance in generating variable-length sentences. The second category is retrieval based
approach, this sort of methods treat image captioning as retrieval task. By leveraging distance metric to retrieve
similar captioned images, then modify and combine retrieved captions to generate caption \cite{kuznetsova2014treetalk}.
But these approaches generally need additional procedures such as modification and generalization process to fit image
query.

Inspired by the success use of CNN \cite{krizhevsky2012imagenet, zeiler2014visualizing} and Recurrent Neural Network
\cite{bahdanau2014neural, mikolov2010recurrent, mikolov2011extensions}. The third category is emerged as neural network
based methods \cite{karpathy2014deep, karpathy2015deep, kiros2014unifying, vinyals2015show, xu2015show}. Our work also
belongs to this category. Among those work, Kiro et al.\cite{kiros2014multimodal} can be as pioneer work to use neural
network for image captioning with multimodal neural language model. In their follow up work \cite{kiros2014unifying},
Kiro et al. introduced an encoder-decoder pipeline where sentence was encoded by LSTM and decoded with structure-content
neural language model (SC-NLM). Socher et al.\cite{socher2014grounded} presented a DT-RNN (Dependency Tree-Recursive
Neural Network) to embed sentence into a vector space in order to retrieve images. Later on, Mao et
al.\cite{mao2014deep} proposed m-RNN which replaces feed-forward neural language model in \cite{kiros2014unifying}.
Similar architectures were introduced in NIC \cite{vinyals2015show} and LRCN \cite{donahue2015long}, both approaches use
LSTM to learn text context.

Our work is based on the LSTM method to do the image captioning. In our approach, we used a pre-trained concolutional
network as the encoder and an LSTM model as the decoder to do the image captioning. We also tried to replaced the LSTM
with the Vanilla RNN and GRU to compared the model performances.
%--------------------------------------------

\section{Methods}


\subsection{Dataset}
For this image captioning task, we used the dataset from the well-known Common Objects in Context\cite{lin2015microsoft}
(COCO) repssitory. COCO is a large-scale object detection, segmentation, and captioning dataset. In this report, we used
a subset (around 1/5) of the COCO 2015 Image Captioning Task. The training set contains around 82k images with roughly
410k captions while the test set has around 3k images with almost 15k captions. The original images in the dataset are
of different sizes and aspect ratios, which we are resizing to 256x256 before the training.


\subsection{Model}
\subsubsection{Baseline Model}
Our captioning system us implemented based on a Long Short-Time Memory (LSTM) network (baseline model). For the encoder
part, we use a forzen pretrained convolutional network, namely ResNet50, as the encoder. We removed the last layer of
the network and added a trainable linear layer that outputs a feaure vector with a fixed size for each image. This
feature becomes the initial hidden state and cell state of the LSTM network. For baseline model, we resized the image to
256x256.
% TODO: Add more detaild on decoder part

\subsubsection{Vanilla RNN and GRU}
For the model comparison, we also tried Vanilla RNN and GRU\cite{cho2014learning}. GRU is a varient of recurrent neural
networks. GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it
lacks an output gate.\cite{enwiki:997015931} For these two models, we simply replaced the LSTM module in the encoder
with either a Vanilla or a GRU, while the others remained to be same. Then, trained and compared the performance of
these three different models.



%--------------------------------------------

\section{Results}

% TODO: Include the best hyperparameters of the fine tuning model
\subsection{Learning Curve}

The learning curve for the Vanilla RNN and GRU could be found in \autoref{fig:loss_train_val_model}.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{VanillaRNN_Loss.png}
    \caption{Vanilla RNN Loss}
    \label{fig:loss_train_baseline}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{GRU_Loss.png}
    \caption{GRU Loss}
    \label{fig:loss_val_baseline}
  \end{subfigure}
  \caption{Training and Validation loss for Vanilla RNN and GRU}
  \label{fig:loss_train_val_model}
\end{figure}



\subsection{Cross Entropy Loss}
The test set Cross Entropy Loss of different could be found in \autoref{table:CrossEntropyLoss}

\begin{table}[h]
  \caption{Cross Entropy Loss for different models}
  \label{table:CrossEntropyLoss}
  \centering
  \begin{tabular}{@{}ll@{}}
    \toprule
    Model             & Testset Cross Entropy Loss \\
    \midrule
    Baseline          & 1.966                      \\
    Vanilla RNN       & 1.819                      \\
    GRU               & 1.586                      \\
    Fine tuning model & TBD                        \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{BLEU Score}
For this report, we set the weight for BLEU1 to be [1, 0, 0, 0] and BLEU4 to be [0.25, 0.25, 0.25, 0.25].
The BLEU scores for different models could be found in \autoref{table:BLEUScores}

\begin{table}[h]
  \caption{BLEU scores for different models}
  \label{table:BLEUScores}
  \centering
  \begin{tabular}{@{}lll@{}}
    \toprule
    Model              & BLEU1 & BLEU4 \\
    \midrule
    Baseline           & 56.48 & 7.75  \\
    Vanilla RNN        & 54.59 & 8.93  \\
    GRU                & 65.16 & 17.17 \\
    Finee tuning model & TBD   & TBD   \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Images Visualization of the Best Performance Model}
% TODO: Report Requirement 10

%--------------------------------------------


\section{Discussion}

\subsection{Baseline with different Temperatures}

\begin{table}[h]
  \caption{BLEU scores for baseline model with different temperature}
  \label{table:StochasticBLEUScores}
  \centering
  \begin{tabular}{@{}lll@{}}
    \toprule
    Temperature & BLEU1 & BLEU4 \\
    \midrule
    0.01        & 0.117 & 0.022 \\
    0.1         & 0.107 & 0.020 \\
    0.2         & 0.099 & 0.018 \\
    0.7         & 0.106 & 0.020 \\
    1           & 0.118 & 0.022 \\
    1.5         & 0.142 & 0.026 \\
    2           & 0.152 & 0.028 \\
    5           & 0.169 & 0.032 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Baseline LSTM vs Vanilla RNN vs GRU}
\subsubsection{Learning Curve}
% TODO: Number of epochs needed for different model

\subsubsection{Cross Entropy Loss}
% TODO: performance differences for different models

\subsubsection{BLEU Score}
% TODO: 

\subsection{Baseline model vs fine tuning model}


\subsubsection{Learning Curve}
% TODO: learning curve difference for fine tuning model

\subsubsection{Cross Entropy Loss}
% TODO: performances differenced for baseline and fine tuning model

\subsubsection{BLEU Score}
% TODO:


%--------------------------------------------

\section{Individual contributions to the project}

\paragraph{Fangzhou Ai}


\paragraph{Yue Qiao}

Baseline LSTM Model

\paragraph{Zunming Zhang}


\bibliographystyle{plainnat}
\bibliography{bibfile}

\end{document}