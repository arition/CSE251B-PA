\documentclass{article}

% if you need to pass options to natbib, use, e.g.: \PassOptionsToPackage{numbers, compress}{natbib} before loading
%     neurips_2020

% ready for submission \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the [preprint] option:
% \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.: \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[preprint]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfigure}

\title{Semantic segmentation using full convolutional network}

% The \author macro works with any number of authors. There are two commands used to separate the names and addresses of
% multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the lines. Using \AND forces a line break at
% that point. So, if LaTeX puts 3 of 4 authors names on the first line, and the last on the second line, try using \AND
% instead of \And before the third author name.

\author{%
  Fangzhou Ai\\
  Department of Electrical and Computer Engineering\\
  University of California, San Diego\\
  La Jolla, CA 92093 \\
  \texttt{faai@eng.ucsd.edu} \\
  % examples of more authors
   \And Yue Qiao \\
   Department of Electrical and Computer Engineering \\
   University of California, San Diego\\
   La Jolla, CA 92093 \\
   \texttt{yuq021@eng.ucsd.edu}\\
   \And Zunming Zhang \\
   Department of Electrical and Computer Engineering \\
   University of California, San Diego\\
   La Jolla, CA 92093 \\
   \texttt{zuz008@eng.ucsd.edu}\\
}

\begin{document}

\maketitle

\begin{abstract}
    TBA
\end{abstract}

%--------------------------------------------

\section{Introduction}

%--------------------------------------------

\section{Related Work}

%--------------------------------------------

\section{Methods}

\subsection{Baseline}

\subsection{Experimentation}

%--------------------------------------------

\section{Results}

\subsection{Baseline model}

\subsection{Improved baseline model}
\subsubsection{Dataset Augmentation}
From the baseline model, we tried to do some data augmentation on the training dataset to see whether the performance could be approved. Since training is quite time-consuming, in this report, we implemented and tested two different data augmentation algorithm and tried to compare the performance with each other.

The data augmentation methods used are:
\begin{itemize}
    \item Mirror flip: for the training dataset, we mirror flipped the dataset and fed them into the model
    \item Image Rotating: rotate the images slightly (for a certain degree), in this report, we tried to rotate the images by 1 degree and then fed them into the model
\end{itemize}

\autoref{table:augmentation} shows the model performance of different augmentation method. From the table, we could see that the performance was improved by ~ 10\% simply by rotating the training dataset for a small angle.

\begin{table}[ht]
    \caption{Comparison of performance of different augmentation methods}
    \label{table:augmentation}
    \centering
    \begin{tabular}{@{}ll@{}ll@{}}
        \toprule
                                                    & Mirror flip           & Rotation       \\ \midrule
        Validation set pivel accuracy               & 0.6953          & 0.7936   \\
        Average IoU                                 & 0.2313          & 0.3393   \\
        IoU for road(0)                             & 0.7477          & 0.8577   \\
        IoU for sidewalk(2)                         & 0.0874          & 0.2483   \\
        IoU for car(9)                              & 0.2262          & 0.4857   \\
        IoU for billboard(17)                       & 0.2542          & 0.2755   \\
        IoU for sky(25)                             & 0.8105          & 0.9445   \\ \bottomrule
    \end{tabular}
\end{table}
The train and validation loss for mirror flip and rotation methods could be seen in \autoref{fig:fliploss} and \autoref{fig:rotationloss}, respectively. The visualization of the segmented output for the first image in the test.csv overlaid on the image for mirror flip and rotation method could be seen in \autoref{fig:mirror_flip} and \autoref{fig:rotation}, respectively.


\begin{figure}
    \centering
    \includegraphics[width=0.90\linewidth]{flip_loss.png}
    \caption{Training and validation loss for flip augmentation method}
    \label{fig:fliploss}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.90\linewidth]{rotate_loss.png}
    \caption{Training and validation loss for Rotation augmentation method}
    \label{fig:rotationloss}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=0.96\linewidth]{flip_method.png}
    \caption{Visualization of segmented output for Mirror flip augmentation method}
    \label{fig:mirror_flip}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.96\linewidth]{rotate_method.png}
    \caption{Visualization of segmented output for Rotation augmentation method}
    \label{fig:rotation}
\end{figure}




\subsubsection{Imbalanced class problem}
For the imbalanced class problem, what we usually do is to pressuring the network to categorize the infrequently seen class. We proposed to use the weighted loss method to accommodate this issue, which will weight infrequent classes more.

For the weighted loss, the idea is to set the weight of the loss of each class inversely proportion to the frequency of each class in the training dataset.
For the data pre-processing, we calculated the total pixel count of each class in all the training dataset and then use this information to help us set the weight of the loss.

Suppose, the number of samples for each class is stored in nSamples, then we use $normedWeights$ = [1 - (x / sum(nSamples)) for x in nSamples] as the normalized weight of the loss for each class.

For this dataset, we calculated the number of samples for each class is:

$ nSamples = [2198453584,519609102,28929892,126362539,58804972,59032134,94293190,2569952, $

$ 101519794,163659019,105075839,47400301,30998126,133835645,135950687,41339482,15989829, $

$ 104795610,8062798,450973,94820222,341805135,557016609,71159311,1465165566,1823922767,2775322] $

Then we could be able to set the weight of the loss for each class according to nSamples.

The train and validation loss after introducing weighted loss method could be seen in \autoref{fig:weightedloss}. The performance of the model could be seen in \autoref{table:weighted} and the visualization of the segmented output for the first image in the testdataset overlaid on the image could be seen in \autoref{fig:weighted}   

\begin{table}[ht]     % need to be changed once finishing training
    \caption{Performance of model after introducing the weighted loss for the imbalanced dataset}
    \label{table:weighted}
    \centering
    \begin{tabular}{@{}ll@{}ll@{}}
        \toprule
                                                    & weighted loss w/ rotation       \\ \midrule
        Validation set pivel accuracy               & 0.6953          \\
        Average IoU                                 & 0.2313          \\
        IoU for road(0)                             & 0.7477          \\
        IoU for sidewalk(2)                         & 0.0874          \\
        IoU for car(9)                              & 0.2262          \\
        IoU for billboard(17)                       & 0.2542          \\
        IoU for sky(25)                             & 0.8105          \\ \bottomrule
    \end{tabular}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.90\linewidth]{rotate_loss.png}   % need to be changed once finishing training
    \caption{Training and validation loss after introducing weighted loss method}
    \label{fig:weightedloss}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.96\linewidth]{rotate_method.png}  % need to be changed once finishing training
    \caption{Visualization of segmented output after introducing weighted loss method}
    \label{fig:weighted}
\end{figure}


\subsection{Custom model}
Inspired by this \href{https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf}{paper}, we replace the encoder part in FCN model with the RRCNN layer instead, the RRCNN represent better abstraction ability, thus we would expect better IoU and precision. However the RRCNN is really memory-intensive, we have to crop the image to only 256 *256 size and delete 3 layers from original model's encoding part, which severly influence the performance of our model. What's worse is the RRCNN encoding part contains maxpooling operation after each RRCNN block, which means the images size would shrinks even more, since we have already started from a relative small size, maxpool will almost ruin all the details in the feature map.
\subsection{Transfer learning}
In this part we employ the DEEPLABV3-RESNET101 model, which is a default pre-trained net work in PyTorch, from the \href{https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/}{official document} we know that this model is constructed by a Deeplabv3 model with a ResNet-101 backbone. The pre-trained model has been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset. We modifier the classifier part so that it could match the number of our feature, and we freeze the parameters of the pretrained part to accelerate the whole procedure.
\subsection{U-Net}
Folowing the routine demostrated in the \href{https://arxiv.org/pdf/1505.04597.pdf}{U-net paper}, we build the U-net by our self. The obvious difference is the U-net convolution block contains 2 convolution of the same size, while the FCN model only contains 1 convolution operation. We notice that the U-net model is also memory intensive while the datahub only give us a GTX 1080 ti with  11GB memory, hence there's a trade-off of between batch size, image size and model's completeness. We decide to crop the image and shrink the batch size to run the whole model, though this might not be the best solution, but this could give us a flavor of how the complete network looks like.

%--------------------------------------------

\section{Discussion}

%--------------------------------------------

\section{Individual contributions to the project}
\label{Sec:ICP}

\paragraph{Fangzhou Ai}
Custom model, Transfer learning and U-Net model.

\paragraph{Yue Qiao}

Evaluation metrics, Dataset loading and Baseline Model.

\paragraph{Zunming Zhang}

Improved baseline model with mirror flip, rotate and weighted loss for imbalanced classes

\end{document}